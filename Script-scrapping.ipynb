{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import pandas as pd\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting jobs links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicked 'Load More' 1 times.\n",
      "Clicked 'Load More' 2 times.\n",
      "Clicked 'Load More' 3 times.\n",
      "Clicked 'Load More' 4 times.\n",
      "Clicked 'Load More' 5 times.\n",
      "Clicked 'Load More' 6 times.\n",
      "Clicked 'Load More' 7 times.\n",
      "Clicked 'Load More' 8 times.\n",
      "Clicked 'Load More' 9 times.\n",
      "Clicked 'Load More' 10 times.\n",
      "Links have been scraped and saved to 'links_sheet.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set the path to the chromedriver executable\n",
    "PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "s = Service(PATH)\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(\"https://www.linkedin.com/jobs/search/?currentJobId=3959479897&distance=25&geoId=105015875&keywords=full%20remote&origin=JOBS_HOME_KEYWORD_HISTORY&refresh=true\")\n",
    "\n",
    "# Wait for the page to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'infinite-scroller__show-more-button')))\n",
    "\n",
    "clicks = 0\n",
    "max_clicks = 10  \n",
    "initial_scrolls = 6  # Scroll down 6 times before trying to click the button\n",
    "scroll_repeat_after_button = 1  # Scroll down 1 time after clicking the \"Load More\" button\n",
    "\n",
    "# Initial scrolling to load content before interacting with the \"Load More\" button\n",
    "for _ in range(initial_scrolls):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    WebDriverWait(driver, 10).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "    time.sleep(2)  # Allow time for the scroll to complete and any new content to load\n",
    "\n",
    "while clicks < max_clicks:\n",
    "    try:\n",
    "        # Wait for and click the \"Load More\" button\n",
    "        load_more = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"infinite-scroller__show-more-button\")))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more)  # Scroll to the \"Load More\" button\n",
    "        load_more.click()\n",
    "        clicks += 1\n",
    "        print(f\"Clicked 'Load More' {clicks} times.\")\n",
    "        \n",
    "        # Scroll down the page once after clicking the button\n",
    "        for _ in range(scroll_repeat_after_button):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            WebDriverWait(driver, 10).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            time.sleep(2)  # Allow time for the scroll to complete and any new content to load\n",
    "    \n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        # If the \"Load More\" button is not found or not interactable, stop scrolling\n",
    "        print(f\"No more 'Load More' button found or exception occurred: {e}\")\n",
    "        break\n",
    "\n",
    "# After finishing the scrolling and clicking process, scrape the links\n",
    "try:\n",
    "    container = driver.find_element(By.CLASS_NAME, 'jobs-search__results-list')\n",
    "    links = container.find_elements(By.TAG_NAME, \"li\")\n",
    "    liste = [element.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") for i, element in enumerate(links) if i <= 200]\n",
    "    df = pd.DataFrame({'Links': liste})\n",
    "    df.to_excel(\"links_sheet.xlsx\", index=False)\n",
    "    print(\"Links have been scraped and saved to 'links_sheet.xlsx'\")\n",
    "except Exception as e:\n",
    "    print(f'Error in getting links: {e}')\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ectracting jobs informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "test2\n",
      "job___________  0\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)']\n",
      "['ambi.careers']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  1\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative']\n",
      "['ambi.careers', 'APOLOP2P']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  2\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  3\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  4\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative', 'Art Director Assistant']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P', '99ads']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/99ads?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  5\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative', 'Art Director Assistant', 'Appointment Setter - Permanent work from home - AU morning shift']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P', '99ads', 'Scale-X Solutions']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/99ads?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/scale-x-solutions?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  6\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative', 'Art Director Assistant', 'Appointment Setter - Permanent work from home - AU morning shift', 'Marketing Director (Digital Marketing Agency)']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P', '99ads', 'Scale-X Solutions', 'Remotivate']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/99ads?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/scale-x-solutions?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/letsremotivate?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  7\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative', 'Art Director Assistant', 'Appointment Setter - Permanent work from home - AU morning shift', 'Marketing Director (Digital Marketing Agency)', 'Comptable full remote']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P', '99ads', 'Scale-X Solutions', 'Remotivate', 'Fabulous']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/99ads?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/scale-x-solutions?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/letsremotivate?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/fabulous-app?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  8\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative', 'Art Director Assistant', 'Appointment Setter - Permanent work from home - AU morning shift', 'Marketing Director (Digital Marketing Agency)', 'Comptable full remote', 'Paid Social Media Specialist']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P', '99ads', 'Scale-X Solutions', 'Remotivate', 'Fabulous', 'Digitsonly']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/99ads?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/scale-x-solutions?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/letsremotivate?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/fabulous-app?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/digitsonly?trk=public_jobs_topcard-org-name']\n",
      "description done\n",
      "job___________  9\n",
      "['Freelance Recruiter (Startups and Scaleups - PARIS Remote)', 'Customer Support Representative', 'Project Manager (Remote, Full Time)', 'Customer Support Representative', 'Art Director Assistant', 'Appointment Setter - Permanent work from home - AU morning shift', 'Marketing Director (Digital Marketing Agency)', 'Comptable full remote', 'Paid Social Media Specialist', 'Remote Staff Accountant (GL)']\n",
      "['ambi.careers', 'APOLOP2P', 'Riess Group', 'APOLOP2P', '99ads', 'Scale-X Solutions', 'Remotivate', 'Fabulous', 'Digitsonly', 'MCI']\n",
      "['https://linkedin.com/company/ambi-careers?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/riessgroup?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/apolop2p?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/99ads?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/scale-x-solutions?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/letsremotivate?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/fabulous-app?trk=public_jobs_topcard-org-name', 'https://linkedin.com/company/digitsonly?trk=public_jobs_topcard-org-name', 'https://www.linkedin.com/company/mci-bpo?trk=public_jobs_topcard-org-name']\n",
      "description done\n"
     ]
    }
   ],
   "source": [
    "# def clean_link(link):\n",
    "#     \"\"\"\n",
    "#     Remove the subdomain prefix (like 'fr.', 'uk.') from the link if it is not 'www.'.\n",
    "\n",
    "#     Args:\n",
    "#     - link (str): The URL to be cleaned.\n",
    "\n",
    "#     Returns:\n",
    "#     - str: The cleaned URL.\n",
    "#     \"\"\"\n",
    "#     # Check if 'www.' is in the link after '://'\n",
    "#     if '://www.' in link:\n",
    "#         return link\n",
    "    \n",
    "#     # Find the position of '://'\n",
    "#     pos = link.find('://') + 3  # Position right after '://'\n",
    "    \n",
    "#     # Split the URL into components\n",
    "#     base_part = link[pos:]\n",
    "    \n",
    "#     # Check if the base part has a subdomain (like 'fr.' or 'uk.')\n",
    "#     if '.' in base_part and not base_part.startswith('www.'):\n",
    "#         # Find the first dot to determine where the subdomain ends\n",
    "#         first_dot = base_part.find('.')\n",
    "#         # Remove the subdomain prefix\n",
    "#         cleaned_url = link[:pos] + base_part[first_dot + 1:]\n",
    "#         return cleaned_url\n",
    "    \n",
    "#     return link\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import time\n",
    "# import pandas as pd\n",
    "\n",
    "# PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "# s = Service(PATH)\n",
    "\n",
    "# # Initialize the Chrome WebDriver\n",
    "# driver = webdriver.Chrome(service=s)\n",
    "# job_links = []\n",
    "# links = pd.read_excel(\"links_sheet.xlsx\")\n",
    "# print(\"test1\")\n",
    "# links_list = links['Links'].tolist()\n",
    "# print(\"test2\")\n",
    "# job_title = []\n",
    "# company_name = []\n",
    "# company_link = []\n",
    "# description = []\n",
    "# Niveau_hierarchique = []\n",
    "# Type_emploi = []\n",
    "# Fonction = []\n",
    "# Secteurs = []\n",
    "\n",
    "# for i, link in enumerate(links_list):\n",
    "#     if i > 20 : \n",
    "#         break\n",
    "#     try:\n",
    "#         driver.get(link)\n",
    "#         time.sleep(5)\n",
    "#         print(\"job___________ \", i)\n",
    "#         container = driver.find_element(By.CLASS_NAME, \"core-rail\")\n",
    "\n",
    "#         TOP_card = container.find_element(By.CLASS_NAME, \"top-card-layout\")\n",
    "#         job_title.append(TOP_card.find_element(By.CLASS_NAME, \"top-card-layout__title\").text)\n",
    "#         print(job_title)\n",
    "#         company_name.append(TOP_card.find_element(By.CLASS_NAME, \"topcard__org-name-link\").text)\n",
    "#         print(company_name)\n",
    "#         company_link.append(clean_link(TOP_card.find_element(By.CLASS_NAME, \"topcard__org-name-link\").get_attribute(\"href\")))\n",
    "#         print(company_link)\n",
    "        \n",
    "#         details_container = container.find_element(By.CLASS_NAME, \"core-section-container__content\")\n",
    "#         description.append(details_container.find_element(By.CLASS_NAME, \"show-more-less-html__markup\").text)\n",
    "#         print(\"description done\")\n",
    "\n",
    "#         job_criterias_container = details_container.find_element(By.CLASS_NAME, \"description__job-criteria-list\")\n",
    "#         job_criteria_list = job_criterias_container.find_elements(By.CLASS_NAME, \"description__job-criteria-item\")\n",
    "        \n",
    "#         # Initialize criteria with 'null'\n",
    "#         criteria_values = {\n",
    "#             'Niveau_hierarchique': 'null',\n",
    "#             'Type_emploi': 'null',\n",
    "#             'Fonction': 'null',\n",
    "#             'Secteurs': 'null'\n",
    "#         }\n",
    "\n",
    "#         for k, criteria in enumerate(job_criteria_list):\n",
    "#             if k == 0:\n",
    "#                 criteria_values['Niveau_hierarchique'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "#             elif k == 1:\n",
    "#                 criteria_values['Type_emploi'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "#             elif k == 2:\n",
    "#                 criteria_values['Fonction'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "#             elif k == 3:\n",
    "#                 criteria_values['Secteurs'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "        \n",
    "#         Niveau_hierarchique.append(criteria_values['Niveau_hierarchique'])\n",
    "#         Type_emploi.append(criteria_values['Type_emploi'])\n",
    "#         Fonction.append(criteria_values['Fonction'])\n",
    "#         Secteurs.append(criteria_values['Secteurs'])\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing job {i}: {e}\")\n",
    "#         job_title.append('null')\n",
    "#         company_name.append('null')\n",
    "#         company_link.append('null')\n",
    "#         description.append('null')\n",
    "#         Niveau_hierarchique.append('null')\n",
    "#         Type_emploi.append('null')\n",
    "#         Fonction.append('null')\n",
    "#         Secteurs.append('null')\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'Job title': job_title, \n",
    "#     'company name': company_name, \n",
    "#     'company link': company_link, \n",
    "#     'Job description': description, \n",
    "#     'Niveau hierarchique': Niveau_hierarchique, \n",
    "#     'Type emploi': Type_emploi, \n",
    "#     'Fonction': Fonction, \n",
    "#     'Secteurs': Secteurs\n",
    "# })\n",
    "\n",
    "# df.to_excel(\"data_sheet.xlsx\", index=False)\n",
    "# driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ****** essai2\n",
    "\n",
    "# import subprocess\n",
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import pandas as pd\n",
    "\n",
    "# def clean_link(link):\n",
    "#     \"\"\"\n",
    "#     Remove the subdomain prefix (like 'fr.', 'uk.') from the link if it is not 'www.'.\n",
    "\n",
    "#     Args:\n",
    "#     - link (str): The URL to be cleaned.\n",
    "\n",
    "#     Returns:\n",
    "#     - str: The cleaned URL.\n",
    "#     \"\"\"\n",
    "#     # Check if 'www.' is in the link after '://'\n",
    "#     if '://www.' in link:\n",
    "#         return link\n",
    "    \n",
    "#     # Find the position of '://'\n",
    "#     pos = link.find('://') + 3  # Position right after '://'\n",
    "    \n",
    "#     # Split the URL into components\n",
    "#     base_part = link[pos:]\n",
    "    \n",
    "#     # Check if the base part has a subdomain (like 'fr.' or 'uk.')\n",
    "#     if '.' in base_part and not base_part.startswith('www.'):\n",
    "#         # Find the first dot to determine where the subdomain ends\n",
    "#         first_dot = base_part.find('.')\n",
    "#         # Remove the subdomain prefix\n",
    "#         cleaned_url = link[:pos] + base_part[first_dot + 1:]\n",
    "#         return cleaned_url\n",
    "    \n",
    "#     return link\n",
    "\n",
    "# def start_new_driver():\n",
    "#     # Path to your ChromeDriver\n",
    "#     PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "#     s = Service(PATH)\n",
    "    \n",
    "#     # Initialize the Chrome WebDriver\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument(\"--remote-debugging-port=9222\")  # This allows multiple instances to run\n",
    "#     driver = webdriver.Chrome(service=s, options=options)\n",
    "#     return driver\n",
    "\n",
    "# links = pd.read_excel(\"links_sheet.xlsx\")\n",
    "# print(\"test1\")\n",
    "# links_list = links['Links'].tolist()\n",
    "# print(\"test2\")\n",
    "# job_title = []\n",
    "# company_name = []\n",
    "# company_link = []\n",
    "# description = []\n",
    "# Niveau_hierarchique = []\n",
    "# Type_emploi = []\n",
    "# Fonction = []\n",
    "# Secteurs = []\n",
    "\n",
    "# for i, link in enumerate(links_list):\n",
    "#     if i > 199 : \n",
    "#         break\n",
    "#     try:\n",
    "#         driver = start_new_driver()\n",
    "#         driver.get(link)\n",
    "#         time.sleep(2)\n",
    "#         print(\"job___________ \", i)\n",
    "#         container = driver.find_element(By.CLASS_NAME, \"core-rail\")\n",
    "\n",
    "#         TOP_card = container.find_element(By.CLASS_NAME, \"top-card-layout\")\n",
    "#         job_title.append(TOP_card.find_element(By.CLASS_NAME, \"top-card-layout__title\").text)\n",
    "#         print(job_title)\n",
    "#         company_name.append(TOP_card.find_element(By.CLASS_NAME, \"topcard__org-name-link\").text)\n",
    "#         print(company_name)\n",
    "#         company_link.append(clean_link(TOP_card.find_element(By.CLASS_NAME, \"topcard__org-name-link\").get_attribute(\"href\")))\n",
    "#         print(company_link)\n",
    "        \n",
    "#         details_container = container.find_element(By.CLASS_NAME, \"core-section-container__content\")\n",
    "#         description.append(details_container.find_element(By.CLASS_NAME, \"show-more-less-html__markup\").text)\n",
    "#         print(\"description done\")\n",
    "\n",
    "#         job_criterias_container = details_container.find_element(By.CLASS_NAME, \"description__job-criteria-list\")\n",
    "#         job_criteria_list = job_criterias_container.find_elements(By.CLASS_NAME, \"description__job-criteria-item\")\n",
    "        \n",
    "#         # Initialize criteria with 'null'\n",
    "#         criteria_values = {\n",
    "#             'Niveau_hierarchique': 'null',\n",
    "#             'Type_emploi': 'null',\n",
    "#             'Fonction': 'null',\n",
    "#             'Secteurs': 'null'\n",
    "#         }\n",
    "\n",
    "#         for k, criteria in enumerate(job_criteria_list):\n",
    "#             if k == 0:\n",
    "#                 criteria_values['Niveau_hierarchique'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "#             elif k == 1:\n",
    "#                 criteria_values['Type_emploi'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "#             elif k == 2:\n",
    "#                 criteria_values['Fonction'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "#             elif k == 3:\n",
    "#                 criteria_values['Secteurs'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "        \n",
    "#         Niveau_hierarchique.append(criteria_values['Niveau_hierarchique'])\n",
    "#         Type_emploi.append(criteria_values['Type_emploi'])\n",
    "#         Fonction.append(criteria_values['Fonction'])\n",
    "#         Secteurs.append(criteria_values['Secteurs'])\n",
    "        \n",
    "#         # Close the current window\n",
    "#         driver.quit()\n",
    "#         time.sleep(2)\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing job {i}: {e}\")\n",
    "#         job_title.append('null')\n",
    "#         company_name.append('null')\n",
    "#         company_link.append('null')\n",
    "#         description.append('null')\n",
    "#         Niveau_hierarchique.append('null')\n",
    "#         Type_emploi.append('null')\n",
    "#         Fonction.append('null')\n",
    "#         Secteurs.append('null')\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'Job title': job_title, \n",
    "#     'company name': company_name, \n",
    "#     'company link': company_link, \n",
    "#     'Job description': description, \n",
    "#     'Niveau hierarchique': Niveau_hierarchique, \n",
    "#     'Type emploi': Type_emploi, \n",
    "#     'Fonction': Fonction, \n",
    "#     'Secteurs': Secteurs\n",
    "# })\n",
    "\n",
    "# df.to_excel(\"data_sheet.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# *************************** essai3\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "def clean_link(link):\n",
    "    \"\"\"\n",
    "    Remove the subdomain prefix (like 'fr.', 'uk.') from the link if it is not 'www.'.\n",
    "\n",
    "    Args:\n",
    "    - link (str): The URL to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - str: The cleaned URL.\n",
    "    \"\"\"\n",
    "    # Check if 'www.' is in the link after '://'\n",
    "    if '://www.' in link:\n",
    "        return link\n",
    "    \n",
    "    # Find the position of '://'\n",
    "    pos = link.find('://') + 3  # Position right after '://'\n",
    "    \n",
    "    # Split the URL into components\n",
    "    base_part = link[pos:]\n",
    "    \n",
    "    # Check if the base part has a subdomain (like 'fr.' or 'uk.')\n",
    "    if '.' in base_part and not base_part.startswith('www.'):\n",
    "        # Find the first dot to determine where the subdomain ends\n",
    "        first_dot = base_part.find('.')\n",
    "        # Remove the subdomain prefix\n",
    "        cleaned_url = link[:pos] + base_part[first_dot + 1:]\n",
    "        return cleaned_url\n",
    "    \n",
    "    return link\n",
    "\n",
    "def start_new_driver():\n",
    "    # Path to your ChromeDriver\n",
    "    PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "    s = Service(PATH)\n",
    "    \n",
    "    # Initialize the Chrome WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--remote-debugging-port=9222\")  # This allows multiple instances to run\n",
    "    driver = webdriver.Chrome(service=s, options=options)\n",
    "    return driver\n",
    "\n",
    "links = pd.read_excel(\"links_sheet.xlsx\")\n",
    "print(\"test1\")\n",
    "links_list = links['Links'].tolist()\n",
    "print(\"test2\")\n",
    "job_title = []\n",
    "company_name = []\n",
    "company_link = []\n",
    "description = []\n",
    "Niveau_hierarchique = []\n",
    "Type_emploi = []\n",
    "Fonction = []\n",
    "Secteurs = []\n",
    "\n",
    "for i, link in enumerate(links_list):\n",
    "    if i > 9 : \n",
    "        break\n",
    "    try:\n",
    "        driver = start_new_driver()\n",
    "        driver.get(link)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Check if the current URL is in the links list\n",
    "        current_url = driver.current_url\n",
    "        if current_url not in links_list:\n",
    "            print(f\"Unexpected URL: {current_url}. Closing window.\")\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "        print(\"job___________ \", i)\n",
    "        container = driver.find_element(By.CLASS_NAME, \"core-rail\")\n",
    "\n",
    "        TOP_card = container.find_element(By.CLASS_NAME, \"top-card-layout\")\n",
    "        job_title.append(TOP_card.find_element(By.CLASS_NAME, \"top-card-layout__title\").text)\n",
    "        print(job_title)\n",
    "        company_name.append(TOP_card.find_element(By.CLASS_NAME, \"topcard__org-name-link\").text)\n",
    "        print(company_name)\n",
    "        company_link.append(clean_link(TOP_card.find_element(By.CLASS_NAME, \"topcard__org-name-link\").get_attribute(\"href\")))\n",
    "        print(company_link)\n",
    "        \n",
    "        details_container = container.find_element(By.CLASS_NAME, \"core-section-container__content\")\n",
    "        description.append(details_container.find_element(By.CLASS_NAME, \"show-more-less-html__markup\").text)\n",
    "        print(\"description done\")\n",
    "\n",
    "        job_criterias_container = details_container.find_element(By.CLASS_NAME, \"description__job-criteria-list\")\n",
    "        job_criteria_list = job_criterias_container.find_elements(By.CLASS_NAME, \"description__job-criteria-item\")\n",
    "        \n",
    "        # Initialize criteria with 'null'\n",
    "        criteria_values = {\n",
    "            'Niveau_hierarchique': 'null',\n",
    "            'Type_emploi': 'null',\n",
    "            'Fonction': 'null',\n",
    "            'Secteurs': 'null'\n",
    "        }\n",
    "\n",
    "        for k, criteria in enumerate(job_criteria_list):\n",
    "            if k == 0:\n",
    "                criteria_values['Niveau_hierarchique'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "            elif k == 1:\n",
    "                criteria_values['Type_emploi'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "            elif k == 2:\n",
    "                criteria_values['Fonction'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "            elif k == 3:\n",
    "                criteria_values['Secteurs'] = criteria.find_element(By.CLASS_NAME, 'description__job-criteria-text').text\n",
    "        \n",
    "        Niveau_hierarchique.append(criteria_values['Niveau_hierarchique'])\n",
    "        Type_emploi.append(criteria_values['Type_emploi'])\n",
    "        Fonction.append(criteria_values['Fonction'])\n",
    "        Secteurs.append(criteria_values['Secteurs'])\n",
    "        \n",
    "        # Close the current window\n",
    "        driver.quit()\n",
    "        time.sleep(2)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing job {i}: {e}\")\n",
    "        job_title.append('null')\n",
    "        company_name.append('null')\n",
    "        company_link.append('null')\n",
    "        description.append('null')\n",
    "        Niveau_hierarchique.append('null')\n",
    "        Type_emploi.append('null')\n",
    "        Fonction.append('null')\n",
    "        Secteurs.append('null')\n",
    "\n",
    "# Ensure all lists are the same length\n",
    "max_length = max(len(job_title), len(company_name), len(company_link), len(description), len(Niveau_hierarchique), len(Type_emploi), len(Fonction), len(Secteurs))\n",
    "\n",
    "job_title += ['null'] * (max_length - len(job_title))\n",
    "company_name += ['null'] * (max_length - len(company_name))\n",
    "company_link += ['null'] * (max_length - len(company_link))\n",
    "description += ['null'] * (max_length - len(description))\n",
    "Niveau_hierarchique += ['null'] * (max_length - len(Niveau_hierarchique))\n",
    "Type_emploi += ['null'] * (max_length - len(Type_emploi))\n",
    "Fonction += ['null'] * (max_length - len(Fonction))\n",
    "Secteurs += ['null'] * (max_length - len(Secteurs))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'company name': company_name, \n",
    "    'company link': company_link,\n",
    "    'Secteurs': Secteurs, \n",
    "    'Job title': job_title, \n",
    "    'Job description': description, \n",
    "    'Niveau hierarchique': Niveau_hierarchique, \n",
    "    'Type emploi': Type_emploi, \n",
    "    'Fonction': Fonction, \n",
    "\n",
    "})\n",
    "\n",
    "df.to_excel(\"data_sheet.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier data_sheet.xlsx\n",
    "df = pd.read_excel(\"data_sheet.xlsx\")\n",
    "\n",
    "# Supprimer les lignes où toutes les valeurs sont 'null'\n",
    "df = df[~df[['Job title', 'company name', 'company link', 'Job description', 'Niveau hierarchique', 'Type emploi', 'Fonction', 'Secteurs']].eq('null').all(axis=1)]\n",
    "\n",
    "# Trier le DataFrame par la colonne \"company name\" alphabétiquement\n",
    "df = df.sort_values(by='company name')\n",
    "\n",
    "# Enregistrer les résultats dans un nouveau fichier Excel\n",
    "df.to_excel(\"data_sheet.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting companies informations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_____ 0\n",
      "company_____ 1\n",
      "company_____ 2\n",
      "company_____ 3\n",
      "company_____ 4\n",
      "company_____ 5\n",
      "company_____ 6\n",
      "company_____ 7\n",
      "company_____ 8\n",
      "company_____ 9\n",
      "Extraction et mise à jour des données terminées.\n"
     ]
    }
   ],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# import re\n",
    "\n",
    "# # Chemin vers ChromeDriver\n",
    "# PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "# s = Service(PATH)\n",
    "\n",
    "# # Charger les données depuis un fichier Excel\n",
    "# data = pd.read_excel(\"data_sheet.xlsx\")\n",
    "# links = data['company link']\n",
    "\n",
    "# # Initialiser les listes pour stocker les données extraites\n",
    "# description = []\n",
    "# site_web = []\n",
    "# secteur = []\n",
    "# taille_entreprise = []\n",
    "# siege_social = []\n",
    "# type_entreprise = []\n",
    "# domaines = []\n",
    "# annee_fondation = []\n",
    "\n",
    "# def normalize_text(text):\n",
    "#     # Supprimer les caractères non alphanumériques et normaliser les espaces\n",
    "#     return re.sub(r'\\W+', '', text).strip().lower()\n",
    "\n",
    "# for i,link in enumerate(links):\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         # Initialiser le WebDriver pour chaque lien\n",
    "#         driver = webdriver.Chrome(service=s)\n",
    "#         driver.get(link)\n",
    "#         WebDriverWait(driver, 12).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))  # Attendre que la page soit complètement chargée\n",
    "\n",
    "#         # Gérer le bouton de fermeture de la fenêtre modale si présent\n",
    "#         try:\n",
    "#             button = WebDriverWait(driver, 8).until(\n",
    "#                 EC.element_to_be_clickable((By.CLASS_NAME, \"modal__dismiss\"))\n",
    "#             )\n",
    "#             button.click()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Aucun bouton de fermeture de modal trouvé: {e}\")\n",
    "\n",
    "#         # Extraire la description\n",
    "#         try:\n",
    "#             description_element = WebDriverWait(driver, 8).until(\n",
    "#                 EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-test-id=\"about-us__description\"]'))\n",
    "#             )\n",
    "#             description.append(description_element.text)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erreur lors de l'extraction de la description: {e}\")\n",
    "#             description.append(\"null\")\n",
    "\n",
    "#         # Extraire plus de détails\n",
    "#         try:\n",
    "#             more_details_container = WebDriverWait(driver, 10).until(\n",
    "#                 EC.visibility_of_element_located((By.CLASS_NAME, \"mt-6\"))\n",
    "#             )\n",
    "\n",
    "#             dt_container = more_details_container.find_elements(By.TAG_NAME, 'dt')\n",
    "#             dd_container = more_details_container.find_elements(By.TAG_NAME, 'dd')\n",
    "\n",
    "#             # Dictionnaires pour stocker les champs et leurs listes correspondantes\n",
    "#             fields = {\n",
    "#                 normalize_text(\"Site web\"): site_web,\n",
    "#                 normalize_text(\"Secteur\"): secteur,\n",
    "#                 normalize_text(\"Taille de l'entreprise\"): taille_entreprise,\n",
    "#                 normalize_text(\"Siège social\"): siege_social,\n",
    "#                 normalize_text(\"Type\"): type_entreprise,\n",
    "#                 normalize_text(\"Domaines\"): domaines,\n",
    "#                 normalize_text(\"Fondée en\"): annee_fondation\n",
    "#             }\n",
    "\n",
    "#             # Extraire les données basées sur les paires dt/dd\n",
    "#             for dt, dd in zip(dt_container, dd_container):\n",
    "#                 key = normalize_text(dt.text)\n",
    "#                 value = dd.text.strip()\n",
    "#                 if key in fields:\n",
    "#                     fields[key].append(value)\n",
    "#                 else:\n",
    "#                     print(f\"Champ non reconnu: {dt.text.strip()}\")\n",
    "\n",
    "#             # S'assurer que tous les champs ont des valeurs (définir à \"null\" si manquant)\n",
    "#             for field in fields:\n",
    "#                 if field not in [normalize_text(dt.text) for dt in dt_container]:\n",
    "#                     fields[field].append(\"null\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Erreur lors de l'extraction des détails: {e}\")\n",
    "#             # Ajouter \"null\" pour chaque champ en cas d'erreur\n",
    "#             site_web.append(\"null\")\n",
    "#             secteur.append(\"null\")\n",
    "#             taille_entreprise.append(\"null\")\n",
    "#             siege_social.append(\"null\")\n",
    "#             type_entreprise.append(\"null\")\n",
    "#             domaines.append(\"null\")\n",
    "#             annee_fondation.append(\"null\")\n",
    "\n",
    "#         # Fermer la fenêtre actuelle\n",
    "#         driver.quit()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors de l'extraction des données pour le lien {link}: {e}\")\n",
    "#         # Ajouter \"null\" pour chaque champ en cas d'erreur\n",
    "#         description.append(\"null\")\n",
    "#         site_web.append(\"null\")\n",
    "#         secteur.append(\"null\")\n",
    "#         taille_entreprise.append(\"null\")\n",
    "#         siege_social.append(\"null\")\n",
    "#         type_entreprise.append(\"null\")\n",
    "#         domaines.append(\"null\")\n",
    "#         annee_fondation.append(\"null\")\n",
    "\n",
    "# # Vérifier les longueurs des listes\n",
    "# lengths = [len(description), len(site_web), len(secteur), len(taille_entreprise), len(siege_social), len(type_entreprise), len(domaines), len(annee_fondation)]\n",
    "# max_length = max(lengths)\n",
    "\n",
    "# # Ajouter des valeurs \"null\" pour égaliser les longueurs des listes\n",
    "# for field in [description, site_web, secteur, taille_entreprise, siege_social, type_entreprise, domaines, annee_fondation]:\n",
    "#     while len(field) < max_length:\n",
    "#         field.append(\"null\")\n",
    "\n",
    "# # Convertir les listes en DataFrame\n",
    "# scraped_data = pd.DataFrame({\n",
    "#     \"Description de l'entreprise\": description,\n",
    "#     'Site web': site_web,\n",
    "#     'Secteur': secteur,\n",
    "#     \"Taille de l'entreprise\": taille_entreprise,\n",
    "#     'Siège social': siege_social,\n",
    "#     'Type': type_entreprise,\n",
    "#     'Domaines': domaines,\n",
    "#     \"Année de fondation\": annee_fondation\n",
    "# })\n",
    "\n",
    "# # Charger les données existantes du fichier Excel\n",
    "# existing_data = pd.read_excel(\"data_sheet.xlsx\")\n",
    "\n",
    "# # Ajouter de nouvelles colonnes aux données existantes\n",
    "# updated_data = pd.concat([existing_data, scraped_data], axis=1)\n",
    "\n",
    "# # Enregistrer les données mises à jour dans le fichier Excel existant\n",
    "# updated_data.to_excel(\"datasheet.xlsx\", index=False)\n",
    "\n",
    "# print(\"Extraction et mise à jour des données terminées.\")\n",
    "\n",
    "# # Quitter le WebDriver\n",
    "# driver.quit()\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Chemin vers ChromeDriver\n",
    "PATH = r\"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "s = Service(PATH)\n",
    "\n",
    "# Charger les données depuis un fichier Excel\n",
    "data = pd.read_excel(\"data_sheet.xlsx\")\n",
    "links = data['company link']\n",
    "\n",
    "# Initialiser les listes pour stocker les données extraites\n",
    "description = []\n",
    "site_web = []\n",
    "secteur = []\n",
    "taille_entreprise = []\n",
    "siege_social = []\n",
    "type_entreprise = []\n",
    "domaines = []\n",
    "annee_fondation = []\n",
    "\n",
    "# Dictionnaire pour stocker les données extraites par lien unique\n",
    "extracted_data = {}\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Supprimer les caractères non alphanumériques et normaliser les espaces\n",
    "    return re.sub(r'\\W+', '', text).strip().lower()\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    print(\"company_____\",i)\n",
    "\n",
    "    if link not in extracted_data:\n",
    "        try:\n",
    "            # Initialiser le WebDriver pour chaque lien\n",
    "            driver = webdriver.Chrome(service=s)\n",
    "            driver.get(link)\n",
    "            WebDriverWait(driver, 12).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))  # Attendre que la page soit complètement chargée\n",
    "\n",
    "            # Gérer le bouton de fermeture de la fenêtre modale si présent\n",
    "            try:\n",
    "                button = WebDriverWait(driver, 8).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"modal__dismiss\"))\n",
    "                )\n",
    "                button.click()\n",
    "            except Exception as e:\n",
    "                print(f\"Aucun bouton de fermeture de modal trouvé: {e}\")\n",
    "\n",
    "            # Extraire la description\n",
    "            try:\n",
    "                description_element = WebDriverWait(driver, 8).until(\n",
    "                    EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-test-id=\"about-us__description\"]'))\n",
    "                )\n",
    "                description_value = description_element.text\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'extraction de la description: {e}\")\n",
    "                description_value = \"null\"\n",
    "\n",
    "            # Initialiser les valeurs par défaut pour les champs\n",
    "            site_web_value = \"null\"\n",
    "            secteur_value = \"null\"\n",
    "            taille_entreprise_value = \"null\"\n",
    "            siege_social_value = \"null\"\n",
    "            type_entreprise_value = \"null\"\n",
    "            domaines_value = \"null\"\n",
    "            annee_fondation_value = \"null\"\n",
    "\n",
    "            # Extraire plus de détails\n",
    "            try:\n",
    "                more_details_container = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.CLASS_NAME, \"mt-6\"))\n",
    "                )\n",
    "\n",
    "                dt_container = more_details_container.find_elements(By.TAG_NAME, 'dt')\n",
    "                dd_container = more_details_container.find_elements(By.TAG_NAME, 'dd')\n",
    "\n",
    "                # Dictionnaires pour stocker les champs et leurs valeurs extraites\n",
    "                fields = {\n",
    "                    normalize_text(\"Site web\"): site_web_value,\n",
    "                    normalize_text(\"Secteur\"): secteur_value,\n",
    "                    normalize_text(\"Taille de l'entreprise\"): taille_entreprise_value,\n",
    "                    normalize_text(\"Siège social\"): siege_social_value,\n",
    "                    normalize_text(\"Type\"): type_entreprise_value,\n",
    "                    normalize_text(\"Domaines\"): domaines_value,\n",
    "                    normalize_text(\"Fondée en\"): annee_fondation_value\n",
    "                }\n",
    "\n",
    "                # Extraire les données basées sur les paires dt/dd\n",
    "                for dt, dd in zip(dt_container, dd_container):\n",
    "                    key = normalize_text(dt.text)\n",
    "                    value = dd.text.strip()\n",
    "                    if key in fields:\n",
    "                        fields[key] = value\n",
    "                    else:\n",
    "                        print(f\"Champ non reconnu: {dt.text.strip()}\")\n",
    "\n",
    "                # Extraire les valeurs\n",
    "                site_web_value = fields[normalize_text(\"Site web\")]\n",
    "                secteur_value = fields[normalize_text(\"Secteur\")]\n",
    "                taille_entreprise_value = fields[normalize_text(\"Taille de l'entreprise\")]\n",
    "                siege_social_value = fields[normalize_text(\"Siège social\")]\n",
    "                type_entreprise_value = fields[normalize_text(\"Type\")]\n",
    "                domaines_value = fields[normalize_text(\"Domaines\")]\n",
    "                annee_fondation_value = fields[normalize_text(\"Fondée en\")]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'extraction des détails: {e}\")\n",
    "\n",
    "            # Stocker les valeurs extraites dans le dictionnaire\n",
    "            extracted_data[link] = {\n",
    "                \"description\": description_value,\n",
    "                \"site_web\": site_web_value,\n",
    "                \"secteur\": secteur_value,\n",
    "                \"taille_entreprise\": taille_entreprise_value,\n",
    "                \"siege_social\": siege_social_value,\n",
    "                \"type_entreprise\": type_entreprise_value,\n",
    "                \"domaines\": domaines_value,\n",
    "                \"annee_fondation\": annee_fondation_value\n",
    "            }\n",
    "\n",
    "            # Fermer la fenêtre actuelle\n",
    "            driver.quit()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction des données pour le lien {link}: {e}\")\n",
    "            # Stocker des valeurs \"null\" en cas d'erreur\n",
    "            extracted_data[link] = {\n",
    "                \"description\": \"null\",\n",
    "                \"site_web\": \"null\",\n",
    "                \"secteur\": \"null\",\n",
    "                \"taille_entreprise\": \"null\",\n",
    "                \"siege_social\": \"null\",\n",
    "                \"type_entreprise\": \"null\",\n",
    "                \"domaines\": \"null\",\n",
    "                \"annee_fondation\": \"null\"\n",
    "            }\n",
    "\n",
    "    # Utiliser les données extraites pour ce lien\n",
    "    data = extracted_data[link]\n",
    "    description.append(data[\"description\"])\n",
    "    site_web.append(data[\"site_web\"])\n",
    "    secteur.append(data[\"secteur\"])\n",
    "    taille_entreprise.append(data[\"taille_entreprise\"])\n",
    "    siege_social.append(data[\"siege_social\"])\n",
    "    type_entreprise.append(data[\"type_entreprise\"])\n",
    "    domaines.append(data[\"domaines\"])\n",
    "    annee_fondation.append(data[\"annee_fondation\"])\n",
    "\n",
    "# Vérifier les longueurs des listes\n",
    "lengths = [len(description), len(site_web), len(secteur), len(taille_entreprise), len(siege_social), len(type_entreprise), len(domaines), len(annee_fondation)]\n",
    "max_length = max(lengths)\n",
    "\n",
    "# Ajouter des valeurs \"null\" pour égaliser les longueurs des listes\n",
    "for field in [description, site_web, secteur, taille_entreprise, siege_social, type_entreprise, domaines, annee_fondation]:\n",
    "    while len(field) < max_length:\n",
    "        field.append(\"null\")\n",
    "\n",
    "# Convertir les listes en DataFrame\n",
    "scraped_data = pd.DataFrame({\n",
    "    \"Description de l'entreprise\": description,\n",
    "    'Site web': site_web,\n",
    "    'Secteur': secteur,\n",
    "    \"Taille de l'entreprise\": taille_entreprise,\n",
    "    'Siège social': siege_social,\n",
    "    'Type': type_entreprise,\n",
    "    'Domaines': domaines,\n",
    "    \"Année de fondation\": annee_fondation\n",
    "})\n",
    "\n",
    "# Charger les données existantes du fichier Excel\n",
    "existing_data = pd.read_excel(\"data_sheet.xlsx\")\n",
    "\n",
    "# Ajouter de nouvelles colonnes aux données existantes\n",
    "updated_data = pd.concat([existing_data, scraped_data], axis=1)\n",
    "\n",
    "# Enregistrer les données mises à jour dans le fichier Excel existant\n",
    "updated_data.to_excel(\"datasheet.xlsx\", index=False)\n",
    "\n",
    "print(\"Extraction et mise à jour des données terminées.\")\n",
    "\n",
    "# Quitter le WebDriver\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
